---
title: "Liz's Peak Bloom Prediction"
author: "Liz Conley"
date: "2026-02-03"
output: pdf_document
lang: en-US
format:
  html:
    embed-resources: true
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, 
                      message = FALSE,
                      fig.align = 'center',
                      out.width = '80%')
```


## Instructions

In this analysis I'll predict the peak bloom date over the next decade for all five locations required by the competition, using #TODO insert method here.

Current method: 
- ridge regression/lasso/elastic net
  - probably elastic net, so we can tune the alpha hyper parameter

- additional data (sunlight/UV exposure, rainfall/precipitation)
  - sunlight data from ([NOAA UV Index](https://www.cpc.ncep.noaa.gov/products/stratosphere/uv_index/uv_annual.shtml))
  - rainfall data from ([NOAA Quantitative Precipitation Forecasts](https://www.wpc.ncep.noaa.gov/qpf/day1-7.shtml))
  
  - build a model off of historic data, as well as this year's data
  - use k-fold CV to detect over fitting
  
  - do a PCA model for max predictive accuracy, in exchange for blackboxing?
  
  
TODO:
- replicate Quetelet's method using cherry blossom data from ghcnd.
- predict a more accurate Temperature^2 threshold using historical data (linear regression or lasso/EN?)
  - use growing degree days or modified growing degree days instead of day from last frost? (research)
  - generalize a threshold value for precipitation/humidity and/or sunlight/cloudiness?

For this demo analysis we are using methods from the _tidyverse_ of R packages.

```{r}
#| eval: false
#install.packages('tidyverse')
library(tidyverse)
```


## Loading the data

The data for the five sites is provided as a simple text file in CSV format.
Each file contains the dates of the peak bloom of the cherry trees at the respective site, alongside the geographical location of the site.

The six columns in each data file are

* _location_ a human-readable location identifier (`string`).
* _lat_ (approximate) latitude of the cherry trees (`double`).
* _long_ (approximate) longitude of the cherry trees (`double`).
* _alt_ (approximate) altitude of the cherry trees (`double`).
* _year_ year of the observation (`integer`).
* *bloom_date* date of peak bloom of the cherry trees (ISO 8601 date `string`). The "peak bloom date" may be defined differently for different sites
* *bloom_doy* days since January 1st of the year until peak bloom (`integer`). January 1st corresponds to `1`.

In R, the data files can be read with `read.csv()` and concatenated with the `bind_rows()` function:

```{r}
cherry <- read.csv("data/washingtondc.csv") |> 
  bind_rows(read.csv("data/liestal.csv")) |> 
  bind_rows(read.csv("data/kyoto.csv")) |> 
  bind_rows(read.csv("data/vancouver.csv")) |> 
  bind_rows(read.csv("data/nyc.csv"))
```


## Loading additional data

We encourage you to find additional publicly-available data that will improve your predictions.
For example, one source of global meteorological data comes from the Global Historical Climatology Network (GHCN), available through the NOAA web API.

To use the web API, you first need a web service token.
You can request this token (free of charge) via <https://www.ncdc.noaa.gov/cdo-web/token>.
Once you have been issued the token, note it somewhere in your code (or make it available through an environment variable):
```{r}
#| echo: false
NOAA_WEB_API_TOKEN <- Sys.getenv("TDhcJeLNXKlnGaNsjwnNkLhwMcdfZJEq")
```

To connect to and use the web API you may use the following R packages:
```{r}
#| eval: false
#install.packages("httr2")
#install.packages("jsonlite")
library(httr2)
library(jsonlite)
```

The stations closest to the sites for the competition with continuously collected maximum temperatures are USC00186350 (Washington D.C.), GME00127786 (Liestal), JA000047759 (Kyoto), CA001108395 (Vancouver) and USW00014732 (New York)
```{r}
NOAA_API_BASE_URL <- "https://www.ncei.noaa.gov/cdo-web/api/v2/data"

# Define the station IDs for the specified locations
stations <- c(
  "washingtondc" = "GHCND:USW00013743",
  "liestal"      = "GHCND:SZ000001940",
  "kyoto"        = "GHCND:JA000047759",
  "vancouver"    = "GHCND:CA001108395",
  "newyorkcity"  = "GHCND:USW00014732"
  )
```

As a simple demonstration, we retrieve the average seasonal maximum daily temperature (in 1/10 Â°C) from these stations using our own `get_temperature()` function, which wraps the `ghcnd_search()` function in the `rnoaa` package. (N.b. `ghcnd_search()` returns a list. Each element of the list corresponds to an element of the `var` argument.)
```{r}
nested_to_tibble <- function (x) {
  # Determine the variable names in the response
  variable_names <- map(x, names) |> 
    unlist(use.names = FALSE) |> 
    unique()
  
  names(variable_names) <- variable_names

  # Reshape the response from a nested list into a table
  map(variable_names, \(i) {
    map(x, \(y) {
      if (is.null(y[[i]])) {
        NA_character_
      } else {
        y[[i]]
      }
    }) |> 
      unlist(use.names = FALSE)
  }) |> 
    as_tibble()
}

get_daily_avg_temp <- function(station_id, start_date, end_date,
                               api_key, base_url, window_size = 300) {
  windows <- seq(as_date(start_date),
                 as_date(end_date) + days(window_size + 1),
                 by = sprintf("%d days", window_size))
  
  batches <- map2(windows[-length(windows)], windows[-1] - days(1), \(from, to) {
    if (from > Sys.Date()) {
      return(NULL)
    }
    response <- tryCatch(
      request(base_url) |> 
        req_headers(token = api_key) |> 
        req_url_query(
          datasetid = "GHCND",
          stationid = station_id,
          datatypeid = "TAVG,TMAX",
          startdate = from,
          enddate = min(as_date(to), Sys.Date()),
          units = "metric",
          limit = 1000
        ) |> 
        req_retry(max_tries = 10) |> 
        req_perform() |> 
        resp_body_json(),
      
      httr2_http = \(cnd) {
        rlang::warn(sprintf("Failed to retrieve data for station %s in time window %s--%s",
                            station_id, from, to),
                    parent = cnd)
        NULL
      })
  })
  
  map(batches, \(x) nested_to_tibble(x$results)) |> 
    list_rbind() |> 
    mutate(date = as_date(date))
}
```

Test case, getting the daily average temperature:
- in Washington DC (station_id = "GHCND:USW00013743")
- from Jan 1 2026 (start_date = 2026-01-01) to today (end_date = Sys.Date())
- with my api key (api_key = "TDhcJeLNXKlnGaNsjwnNkLhwMcdfZJEq")
- with the given base_url (base_url = "https://www.ncei.noaa.gov/cdo-web/api/v2/data")
```{r}
DC_Daily_Avg_Temp <- get_daily_avg_temp(station_id = "GHCND:USW00013743",
                                        start_date = 2026-01-01,
                                        end_date = Sys.Date(),
                                        api_key = "TDhcJeLNXKlnGaNsjwnNkLhwMcdfZJEq",
                                        base_url = "https://www.ncei.noaa.gov/cdo-web/api/v2/data")

#got data from July 18, 1975 to today for some reason? Can troubleshoot function, or just filter out only this year's data.
```

For my use case, I want to pull: 
- temperature data ("TAVG", "TMAX")
- precipitation/humidity data ("PRCP", "RHAV")
- sunlight/cloudiness data ("TSUN", "ACSC")

(And I'd like to get each of these as separate columns, instead of factors in the datatype column.
```{r}
get_daily_data <- function(station_id, start_date, end_date,
                               api_key, base_url, window_size = 300) {
  windows <- seq(as_date(start_date),
                 as_date(end_date) + days(window_size + 1),
                 by = sprintf("%d days", window_size))
  
  batches <- map2(windows[-length(windows)], windows[-1] - days(1), \(from, to) {
    if (from > Sys.Date()) {
      return(NULL)
    }
    response <- tryCatch(
      request(base_url) |> 
        req_headers(token = api_key) |> 
        req_url_query(
          datasetid = "GHCND",
          stationid = station_id,
          datatypeid = "TAVG,TMAX,PRCP,RHAV,TSUN,ACSC",
          startdate = from,
          enddate = min(as_date(to), Sys.Date()),
          units = "metric",
          limit = 1000
        ) |> 
        req_retry(max_tries = 10) |> 
        req_perform() |> 
        resp_body_json(),
      
      httr2_http = \(cnd) {
        rlang::warn(sprintf("Failed to retrieve data for station %s in time window %s--%s",
                            station_id, from, to),
                    parent = cnd)
        NULL
      })
  })
  
  map(batches, \(x) nested_to_tibble(x$results)) |> 
    list_rbind() |> 
    mutate(date = as_date(date))
}
```

Let's test my version of the function:
```{r}
DC_Daily_Data <- get_daily_data(station_id = "GHCND:USW00013743",
                                        start_date = 2026-01-01,
                                        end_date = Sys.Date(),
                                        api_key = "TDhcJeLNXKlnGaNsjwnNkLhwMcdfZJEq",
                                        base_url = "https://www.ncei.noaa.gov/cdo-web/api/v2/data")
```

# Predicting the peak bloom

One method for predicting peak bloom is using Quetelet's law of the flowering plants. I'll reproduce it here for the cherry blossoms. Used (this article)[https://realworlddatascience.net/applied-insights/tutorials/posts/2023/04/13/flowers.html] to make the code.
```{r}
library("knitr")
#install.packages("kableExtra")
library("kableExtra")
library("tidyverse")
#install.packages("plotly")
library("plotly")
#install.packages('rnoaa', repos = c('https://ropensci.r-universe.dev', 'https://cloud.r-project.org'))
library("rnoaa")
#install.packages("rnpn")
library("rnpn")
library("rvest")

temp_dc <- 
  ghcnd_search(stationid = "USW00013743",
               var = c("tmax", "tmin"),
               date_min = "2025-01-01",
               date_max = "2025-12-31") %>%
  reduce(left_join) %>%
  transmute(year = parse_number(format(date, "%Y")), 
            date, 
            tmax = tmax / 10, 
            tmin = tmin / 10, 
            temp = (tmax + tmin) / 2)

doy_last_frost <- function(tmax, doy_max = 100) {
  dof <- which(tmax[1:doy_max] <= 0)
  if(length(dof) == 0) 1 else max(dof) + 1
}

doy_prediction <- function(temp, tmax)
  doy_last_frost(tmax) + which.max(cumsum(pmax(temp[(doy_last_frost(tmax) + 1):365], 0, na.rm = TRUE)^2) > 4264)


bloom_day_dc <-
  temp_dc %>%
  summarize(date = doy_prediction(temp, tmax) + as.Date("2025-01-01")) %>%
  pull(date)

frost_day_dc <- 
  temp_dc %>% 
  pull(tmax) %>% 
  doy_last_frost() + as.Date("2025-01-01") 

tibble(`last frost date` = frost_day_dc, 
       `bloom date` = bloom_day_dc) %>%
  kable(align = "c",
        caption = "Last frost date and cherry blossom bloom date in Washington, DC in 2025.") %>%
  kable_styling()

```

Modifications from Quetelet's method:
1) Predict a more accurate cumulative square bloom temp. from historical data using LASSO/EN.
2) Use Modified Growing Degree Days (MGDDs) instead of days from last frost?


#Predicting a more accurate square temperature threshold
```{r}
library(caret)
library(glmnet)

#Step 1: get bloom date for each year
cherry_dc <- read.csv("data/washingtondc.csv") #bloom_date & bloom_doy in DC 1921-2024

#step 2: get daily temperature data for each year
temp_dc <- 
  ghcnd_search(stationid = "USW00013743",
               var = c("tmax", "tmin"),
               date_min = "1921-01-01",
               date_max = "2024-12-31") %>%
  reduce(left_join) %>%
  transmute(year = parse_number(format(date, "%Y")), 
            date, 
            tmax = tmax / 10, 
            tmin = tmin / 10, 
            temp = (tmax + tmin) / 2)
# only has data from start of 1942
```

```{r}
#Step 3: calculate sum square temperature at bloom date of each year

#Let's try this for just 1942
temp_1942 <- temp_dc %>%
  filter(year == 1942)

bloom_doy_1942 <- cherry_dc %>%
  select(location, year, bloom_date, bloom_doy) %>% filter(year == 1942)

bloom_date_1942 <- cherry_dc %>%
  select(location, year, bloom_date, bloom_doy) %>% filter(year == 1942)

frost_doy_1942 <- 
  temp_1942 %>% 
  pull(tmax) %>% 
  doy_last_frost()

#is DOY zero or one based? I.e. is Jan. 1 DOY 0 or DOY 1?
#if Jan. 1 is DOY 0, origin = year-01-01
#if Jan. 1 is DOY 1, origin = (year-1)-12-31
doy_to_date <- function(doy, year) {
  origin <- paste(as.character(year), "01-01", sep = "-")
  date <- as_date(doy, origin)
}

frost_date_1942 <- 
  doy_to_date(frost_doy_1942, 1942)

bloom_date_1942 <-
  as_date(bloom_date_1942$bloom_date)


bloom_threshold_temperatures_1942 <- temp_dc %>%
  filter(year == 1942) %>%
  filter(between(date, frost_date_1942, bloom_date_1942))

bloom_thresh_temp_sum_sq <-
  sum(bloom_threshold_temperatures_1942$temp)^2
  
```

#All-in-1 function to get bloom temp threshold by year & location
```{r}
cherry <- read.csv("data/washingtondc.csv") |> 
  bind_rows(read.csv("data/liestal.csv")) |> 
  bind_rows(read.csv("data/kyoto.csv")) |> 
  bind_rows(read.csv("data/vancouver.csv")) |> 
  bind_rows(read.csv("data/nyc.csv"))

#parameters:
# year as a vector (even if it's just 1 year
# location as one of: "washingtondc", "liestal", "kyoto", "vancouver", "newyorkcity"
# stID as the corresponding station ID: "USC00186350" (Washington D.C.), "GME00127786" (Liestal), "JA000047759" (Kyoto), "CA001108395" (Vancouver) and "USW00014732" (New York)

bloom_temp_thresh <- function(yr, loc, stID) {
  cherry_thresh <- cherry %>% filter(location == loc) %>% filter(year %in% yr)
  
  cherry_temps <- ghcnd_search(stationid = stID,
               var = c("tmax", "tmin"),
               date_min = paste(as.character(min(yr)), "01-01", sep = "-"),
               date_max = paste(as.character(max(yr)), "12-31", sep = "-")) %>%
  reduce(left_join) %>%
  transmute(year = parse_number(format(date, "%Y")), 
            date, 
            tmax = tmax / 10, 
            tmin = tmin / 10, 
            temp = (tmax + tmin) / 2)
}


# todo, make it work with multiple inputs
cherry_thresh <- cherry %>% filter(location == c("washingtondc", "kyoto")) %>% filter(year %in% c(1942:1948))

cherry_temps <- ghcnd_search(stationid = c("USW00013743", "JA000047759"),
               var = c("tmax", "tmin"),
               date_min = paste(as.character(min(c(1942:1948))), "01-01", sep = "-"),
               date_max = paste(as.character(max(c(1942:1948))), "12-31", sep = "-")) %>%
  reduce(left_join) %>%
  transmute(year = parse_number(format(date, "%Y")), 
            date, 
            tmax = tmax / 10, 
            tmin = tmin / 10, 
            temp = (tmax + tmin) / 2)


```


```{r}
#LASSO
set.seed(8675309)
cv_5 = trainControl(method = "cv", number = 5)

threshold_elnet = train(
  #Salary ~ ., data = Hitters,
  method = "glmnet",
  trControl = cv_5
)
```

